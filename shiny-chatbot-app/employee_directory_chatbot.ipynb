{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Employee Directory Chatbot\n",
        "\n",
        "Use this Databricks notebook to provision the `employees` table and run a SQL-aware chatbot that answers questions about your employee directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "1. Run the `employees_table.sql` script (included in this repository) or execute the setup cell below to create and seed the `employees` table.\n",
        "2. Deploy or reuse a Databricks Model Serving endpoint that supports Chat Completions (for example, DBRX Instruct).\n",
        "3. Grant this notebook permission to query a SQL warehouse or Unity Catalog schema that holds the employees table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMMAND ----------\n",
        "# Widget-driven configuration so the notebook can run in multiple environments.\n",
        "import os\n",
        "\n",
        "try:\n",
        "    dbutils\n",
        "except NameError:  # pragma: no cover\n",
        "    from pyspark.dbutils import DBUtils\n",
        "    dbutils = DBUtils(spark)\n",
        "\n",
        "def ensure_widget(name: str, default: str, label: str) -> None:\n",
        "    if name not in dbutils.widgets.getArgumentNames():\n",
        "        dbutils.widgets.text(name, default, label)\n",
        "\n",
        "ensure_widget(\"catalog_name\", \"main\", \"Unity Catalog (or hive_metastore)\")\n",
        "ensure_widget(\"schema_name\", \"default\", \"Schema\")\n",
        "ensure_widget(\"serving_endpoint_name\", \"\", \"Serving Endpoint Name\")\n",
        "ensure_widget(\"table_name\", \"employees\", \"Employees Table\")\n",
        "\n",
        "CATALOG = dbutils.widgets.get(\"catalog_name\") or \"main\"\n",
        "SCHEMA = dbutils.widgets.get(\"schema_name\") or \"default\"\n",
        "TABLE = dbutils.widgets.get(\"table_name\") or \"employees\"\n",
        "SERVING_ENDPOINT = dbutils.widgets.get(\"serving_endpoint_name\") or os.getenv(\"SERVING_ENDPOINT\")\n",
        "\n",
        "if not SERVING_ENDPOINT:\n",
        "    raise ValueError(\"Set the `serving_endpoint_name` widget or SERVING_ENDPOINT env var.\")\n",
        "\n",
        "EMPLOYEE_TABLE_FQN = f\"{CATALOG}.{SCHEMA}.{TABLE}\" if CATALOG else f\"{SCHEMA}.{TABLE}\"\n",
        "print(f\"Using table: {EMPLOYEE_TABLE_FQN}\")\n",
        "print(f\"Serving endpoint: {SERVING_ENDPOINT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMMAND ----------\n",
        "# (Optional) Create and seed the employees table directly from the notebook.\n",
        "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
        "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
        "spark.sql(f\"USE {SCHEMA}\")\n",
        "spark.sql(\n",
        "    f'''\n",
        "    CREATE TABLE IF NOT EXISTS {EMPLOYEE_TABLE_FQN} (\n",
        "      id INT GENERATED ALWAYS AS IDENTITY,\n",
        "      name STRING,\n",
        "      department STRING,\n",
        "      role STRING,\n",
        "      start_date DATE\n",
        "    )\n",
        "    '''\n",
        ")\n",
        "spark.sql(f\"DELETE FROM {EMPLOYEE_TABLE_FQN}\")\n",
        "spark.sql(\n",
        "    f'''\n",
        "    INSERT INTO {EMPLOYEE_TABLE_FQN} (name, department, role, start_date) VALUES\n",
        "      ('Alice', 'Data Analytics', 'Engineer', DATE '2022-01-15'),\n",
        "      ('Bob', 'Data Science', 'Analyst', DATE '2023-03-10'),\n",
        "      ('Carol', 'IT', 'Support', DATE '2021-07-22')\n",
        "    '''\n",
        ")\n",
        "display(spark.sql(f\"SELECT * FROM {EMPLOYEE_TABLE_FQN}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMMAND ----------\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Dict, List\n",
        "\n",
        "from mlflow.deployments import get_deploy_client\n",
        "\n",
        "deploy_client = get_deploy_client(\"databricks\")\n",
        "SQL_SCHEMA_DESCRIPTION = f\"Table {EMPLOYEE_TABLE_FQN} columns: id (INT, identity primary key), name (STRING), department (STRING), role (STRING), start_date (DATE). Sample rows include Alice (Data Analytics Engineer), Bob (Data Science Analyst), and Carol (IT Support).\"\n",
        "SQL_PLANNER_SYSTEM_PROMPT = (\n",
        "    \"You are an expert Databricks SQL planner. Given a user question and chat history, produce a safe SELECT query that only reads from the employees table.\"\n",
        "    f\" Use the fully qualified name {EMPLOYEE_TABLE_FQN}.\"\n",
        "    \" Return strict JSON with keys: sql_query (string), reasoning (string), requires_clarification (bool), clarification_prompt (string).\"\n",
        "    \" If the answer cannot be determined, set requires_clarification=true and explain.\"\n",
        "    \" Never perform write operations.\"\n",
        "    f\" Table schema: {SQL_SCHEMA_DESCRIPTION}. Provide only JSON.\"\n",
        ")\n",
        "ANSWER_SYSTEM_PROMPT = (\n",
        "    \"You are an HR assistant who explains employee data retrieved from Databricks SQL. \"\n",
        "    \"Given the question, SQL query, and rows, respond conversationally and cite available employee names. If the rows list is empty, say that no employees matched.\"\n",
        ")\n",
        "chat_history: List[Dict[str, str]] = []\n",
        "\n",
        "def call_llm(messages: List[Dict[str, str]], temperature: float = 0.1, max_tokens: int = 400) -> str:\n",
        "    response = deploy_client.predict(\n",
        "        endpoint=SERVING_ENDPOINT,\n",
        "        inputs={\"messages\": messages, \"temperature\": temperature, \"max_tokens\": max_tokens}\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "def plan_sql(question: str) -> Dict[str, str]:\n",
        "    payload = {\"question\": question, \"history\": chat_history}\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SQL_PLANNER_SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": json.dumps(payload)}\n",
        "    ]\n",
        "    content = call_llm(messages, temperature=0.0)\n",
        "    try:\n",
        "        return json.loads(content)\n",
        "    except json.JSONDecodeError as exc:\n",
        "        raise ValueError(f\"Planner returned non-JSON payload: {content}\") from exc\n",
        "\n",
        "def run_employee_query(sql_query: str) -> List[Dict[str, str]]:\n",
        "    df = spark.sql(sql_query)\n",
        "    return [row.asDict(recursive=True) for row in df.collect()]\n",
        "\n",
        "def craft_answer(question: str, sql_query: str, rows: List[Dict[str, str]]) -> str:\n",
        "    payload = {\"question\": question, \"sql_query\": sql_query, \"rows\": rows}\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": ANSWER_SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": json.dumps(payload, default=str)}\n",
        "    ]\n",
        "    return call_llm(messages, temperature=0.2, max_tokens=350).strip()\n",
        "\n",
        "def ask_employee_bot(question: str) -> Dict[str, str]:\n",
        "    plan = plan_sql(question)\n",
        "    if plan.get(\"requires_clarification\"):\n",
        "        clarification = plan.get(\"clarification_prompt\", \"Could you clarify your request?\")\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": clarification})\n",
        "        return {\"answer\": clarification, \"sql_query\": None, \"rows\": []}\n",
        "\n",
        "    sql_query = plan.get(\"sql_query\")\n",
        "    if not sql_query:\n",
        "        raise ValueError(f\"Planner did not return sql_query: {plan}\")\n",
        "\n",
        "    rows = run_employee_query(sql_query)\n",
        "    answer = craft_answer(question, sql_query, rows)\n",
        "    timestamp = datetime.utcnow().isoformat()\n",
        "    chat_history.extend([\n",
        "        {\"role\": \"user\", \"content\": question, \"timestamp\": timestamp},\n",
        "        {\"role\": \"assistant\", \"content\": answer, \"timestamp\": timestamp}\n",
        "    ])\n",
        "    return {\"answer\": answer, \"sql_query\": sql_query, \"rows\": rows, \"plan\": plan}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "        # COMMAND ----------\n",
        "        # Example chat interactions. Replace the sample questions with user input or drive from a UI widget.\n",
        "        sample_questions = [\n",
        "            \"Who works in Data Analytics?\",\n",
        "            \"List employees hired after 2022.\"\n",
        "        ]\n",
        "\n",
        "        for question in sample_questions:\n",
        "            result = ask_employee_bot(question)\n",
        "            print(f\"\n",
        "Question: {question}\")\n",
        "            print(f\"SQL: {result['sql_query']}\")\n",
        "            print(f\"Answer: {result['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMMAND ----------\n",
        "# Inspect the running conversation history (optional).\n",
        "display(chat_history)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}